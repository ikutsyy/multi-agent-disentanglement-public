import multiprocessing

import torch
from disentanglement.message_level.model_parameters import  SAVE_NAME as S, NUM_WORKERS as OTHERNUM
MULTIPROCESSING_METHOD = 'fork'
# CUDA
CUDA = torch.cuda.is_available()
if CUDA and multiprocessing.get_start_method(allow_none=True) is None:
    multiprocessing.set_start_method(MULTIPROCESSING_METHOD)

# Model Paramters:

DISCRIMINATOR_SIZE = 64

AGENT_BEHAVIOR_SIZE = 4
_DEFAULT_SIZE = 128
ENCODER_HIDDEN1_SIZE = _DEFAULT_SIZE
ENCODER_HIDDEN2_SIZE = _DEFAULT_SIZE
ENCODER_HIDDEN3_SIZE = _DEFAULT_SIZE
ENCODER_HIDDEN4_SIZE = _DEFAULT_SIZE
ENCODER_HIDDEN5_SIZE = _DEFAULT_SIZE
ENCODER_HIDDEN_KNOWLEDGE_SIZE = _DEFAULT_SIZE
ENCODER_HIDDEN_MESSAGE_SIZE = _DEFAULT_SIZE

DECODER_HIDDEN1_SIZE = _DEFAULT_SIZE//4
DECODER_HIDDEN2_SIZE = _DEFAULT_SIZE//4
DECODER_HIDDEN3_SIZE = _DEFAULT_SIZE//4
DECODER_HIDDEN4_SIZE = _DEFAULT_SIZE//4
DECODER_POSITION_HIDDEN_SIZE = _DEFAULT_SIZE//4

KNOWLEDGE_SIZE = 128
TRAIN_CUTOFF = 50000000000000


# Processor parameters
PREPROCESS_CORES = 8

# Data parameters
SAVE_NAME = S
DATASET_NAME = SAVE_NAME+"_dataset.pkl"
PARAMS_FILE = "../../data/params.json"

# Training Parameters:
DATA_PATH = "../../data"
NUM_WORKERS = OTHERNUM


BATCH_SIZE = 64
LABEL_FRACTION = 1.0
LEARNING_RATE = 1e-3
EPS = 1e-9
BIAS_TRAIN = (60000 - 1) / (BATCH_SIZE - 1)
BIAS_TEST = (10000 - 1) / (BATCH_SIZE - 1)

NUM_SAMPLES = 4

t_weights={"G":100}
